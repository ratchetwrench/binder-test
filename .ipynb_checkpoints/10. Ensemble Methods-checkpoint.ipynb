{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Performance with Ensembles \n",
    "Ensembles can give you a boost in accuracy on your dataset. In this chapter you will discover how you can create some of the most powerful types of ensembles in Python using scikit-learn. This lesson will step you through Boosting, Bagging and Majority Voting and show you how you can continue to ratchet up the accuracy of the models on your own datasets. After completing this lesson, you will know: \n",
    "1.\tHow to use bagging ensemble methods such as bagged decision trees, random forest and extra trees. \n",
    "2.\tHow to use boosting ensemble methods such as AdaBoost and stochastic gradient boosting. \n",
    "3.\tHow to use voting ensemble methods to combine the predictions from multiple algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End Result\n",
    "Ensembles Methods\n",
    "\t\tStandard Deviations\n",
    "\tMean\t(+/-) 99%\t(+/-) 95%\t(+/-) 91%\n",
    "Bagging\t\t\t\t\n",
    "Decision Tree\t\t\t\t\n",
    "Random Forest\t\t\t\t\n",
    "Extra Trees\t\t\t\t\n",
    "\t\t\t\t\n",
    "Boosting\t\t\t\t\n",
    "AdaBoost\t\t\t\t\n",
    "Stochastic Gradient Boosting\t\t\t\t\n",
    "\t\t\t\t\n",
    "Voting\t\t\t\t\n",
    "Model 1\t\t\t\t\n",
    "Model 2\t\t\t\t\n",
    "Model 3\t\t\t\t\n",
    "\t\t\t\t\n",
    "Best estimator\t{*.best_estimator_}\t\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Models into Ensemble Predictions \n",
    "The three most popular methods for combining the predictions from different models are: \n",
    "- **Bagging**. Building multiple models (typically of the same type) from different subsamples of the training dataset. \n",
    "- **Boosting**. Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the sequence of models. \n",
    "- **Voting**. Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions. \n",
    "\n",
    "This assumes you are generally familiar with machine learning algorithms and ensemble methods and will not go into the details of how the algorithms work or their parameters. The Pima Indians onset of Diabetes dataset is used to demonstrate each algorithm. Each ensemble algorithm is demonstrated using 10-fold cross validation and the classification accuracy performance metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Algorithms \n",
    "Bootstrap Aggregation (or Bagging) involves taking multiple samples from your training dataset (with replacement) and training a model for each sample. The final output prediction is averaged across the predictions of all of the sub-models. The three bagging models covered in this section are as follows: \n",
    "- Bagged Decision Trees. \n",
    "- Random Forest.\n",
    "- Extra Trees. \n",
    "\n",
    "**Bagged Decision Trees**\n",
    "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning. In the example below is an example of using the BaggingClassifier with the Classification and Regression Trees algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df = load_iris()\n",
    "y = df.target\n",
    "X = df.data\n",
    "\n",
    "cart = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(base_estimator=cart, \n",
    "                          n_estimators=100,\n",
    "                          random_state=42) \n",
    "results = cross_val_score(model, X, y, cv=5)\n",
    "print(results.mean())    # 0.770745044429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "Random Forests is an extension of bagged decision trees. Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of each tree, only a random subset of features are considered for each split. You can construct a Random Forest model for classification using the RandomForestClassifier class2. The example below demonstrates using Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = load_iris()\n",
    "y = df.target\n",
    "X = df.data\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(results.mean())    # 0.770727956254\n",
    "\n",
    "Extra Trees \n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset. You can construct an Extra Trees model for classification using the ExtraTreesClassifier class3. The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Trees Classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = load_iris()\n",
    "y = df.target\n",
    "X = df.data\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print(results.mean())    # 0.760269993165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms \n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence. Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction. The two most common boosting ensemble machine learning algorithms are: \n",
    "- AdaBoost.\n",
    "- Stochastic Gradient Boosting. \n",
    "\n",
    "â€¦"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
