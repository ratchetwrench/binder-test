{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# load dataset\n",
    "filename = pima-indians-diabetes.data.csv\n",
    "names=[preg, plas, pres, skin, test, mass, pedi, age, class] dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(( LR , LogisticRegression()))\n",
    "models.append(( LDA , LinearDiscriminantAnalysis()))\n",
    "models.append(( KNN , KNeighborsClassifier()))\n",
    "models.append(( CART , DecisionTreeClassifier()))\n",
    "models.append(( NB , GaussianNB()))\n",
    "models.append(( SVM , SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = accuracy\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=7)\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle( Algorithm Comparison )\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Model selection and evaluation\n",
    "\n",
    "3.1. Cross-validation: evaluating estimator performance\n",
    "3.1.1. Computing cross-validated metrics\n",
    "3.1.1.1. The cross_validate function and multiple metric evaluation\n",
    "3.1.1.2. Obtaining predictions by cross-validation\n",
    "3.1.2. Cross validation iterators\n",
    "3.1.2.1. Cross-validation iterators for i.i.d. data\n",
    "3.1.2.1.1. K-fold\n",
    "3.1.2.1.2. Repeated K-Fold\n",
    "3.1.2.1.3. Leave One Out (LOO)\n",
    "3.1.2.1.4. Leave P Out (LPO)\n",
    "3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split\n",
    "3.1.2.2. Cross-validation iterators with stratification based on class labels.\n",
    "3.1.2.2.1. Stratified k-fold\n",
    "3.1.2.2.2. Stratified Shuffle Split\n",
    "3.1.2.3. Cross-validation iterators for grouped data.\n",
    "3.1.2.3.1. Group k-fold\n",
    "3.1.2.3.2. Leave One Group Out\n",
    "3.1.2.3.3. Leave P Groups Out\n",
    "3.1.2.3.4. Group Shuffle Split\n",
    "3.1.2.4. Predefined Fold-Splits / Validation-Sets\n",
    "3.1.2.5. Cross validation of time series data\n",
    "3.1.2.5.1. Time Series Split\n",
    "3.1.3. A note on shuffling\n",
    "3.1.4. Cross validation and model selection\n",
    "\n",
    "3.2. Tuning the hyper-parameters of an estimator\n",
    "3.2.1. Exhaustive Grid Search\n",
    "3.2.2. Randomized Parameter Optimization\n",
    "3.2.3. Tips for parameter search\n",
    "3.2.3.1. Specifying an objective metric\n",
    "3.2.3.2. Specifying multiple metrics for evaluation\n",
    "3.2.3.3. Composite estimators and parameter spaces\n",
    "3.2.3.4. Model selection: development and evaluation\n",
    "3.2.3.5. Parallelism\n",
    "3.2.3.6. Robustness to failure\n",
    "3.2.4. Alternatives to brute force parameter search\n",
    "\n",
    "3.3. Model evaluation: quantifying the quality of predictions\n",
    "3.3.1. The scoring parameter: defining model evaluation rules\n",
    "3.3.1.1. Common cases: predefined values\n",
    "3.3.1.2. Defining your scoring strategy from metric functions\n",
    "3.3.1.3. Implementing your own scoring object\n",
    "3.3.1.4. Using multiple metric evaluation\n",
    "3.3.2. Classification metrics\n",
    "3.3.2.1. From binary to multiclass and multilabel\n",
    "3.3.2.2. Accuracy score\n",
    "3.3.2.3. Cohen’s kappa\n",
    "3.3.2.4. Confusion matrix\n",
    "3.3.2.5. Classification report\n",
    "3.3.2.6. Hamming loss\n",
    "3.3.2.7. Jaccard similarity coefficient score\n",
    "3.3.2.8. Precision, recall and F-measures\n",
    "3.3.2.8.1. Binary classification\n",
    "3.3.2.8.2. Multiclass and multilabel classification\n",
    "3.3.2.9. Hinge loss\n",
    "3.3.2.10. Log loss\n",
    "3.3.2.11. Matthews correlation coefficient\n",
    "3.3.2.12. Receiver operating characteristic (ROC)\n",
    "3.3.2.13. Zero one loss\n",
    "3.3.2.14. Brier score loss\n",
    "3.3.3. Multilabel ranking metrics\n",
    "3.3.3.1. Coverage error\n",
    "3.3.3.2. Label ranking average precision\n",
    "3.3.3.3. Ranking loss\n",
    "3.3.4. Regression metrics\n",
    "3.3.4.1. Explained variance score\n",
    "3.3.4.2. Mean absolute error\n",
    "3.3.4.3. Mean squared error\n",
    "3.3.4.4. Mean squared logarithmic error\n",
    "3.3.4.5. Median absolute error\n",
    "3.3.4.6. R² score, the coefficient of determination\n",
    "3.3.5. Clustering metrics\n",
    "3.3.6. Dummy estimators\n",
    "\n",
    "3.4. Model persistence\n",
    "3.4.1. Persistence example\n",
    "3.4.2. Security & maintainability limitations\n",
    "\n",
    "3.5. Validation curves: plotting scores to evaluate models\n",
    "3.5.1. Validation curve\n",
    "3.5.2. Learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Classifier Comparison](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#classifier-comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
